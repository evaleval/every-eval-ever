name: Weekly Evaluation Data Processing and Statistics

on:
  schedule:
    # Weekly: Monday 03:00 UTC
    - cron: '0 3 * * 1'
  workflow_dispatch: {}

jobs:
  scrape:
    name: Step 1 - Scrape and Process HELM Evaluation Data
    runs-on: ubuntu-latest
    timeout-minutes: 1440  # 24 hours - increased for processing all benchmarks in parallel

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup environment (python, deps, caching)
        uses: ./.github/actions/setup-environment

      - name: Run scraping and processing
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO_ID: evaleval/every_eval_ever
          PYTHONUNBUFFERED: 1
          OMP_NUM_THREADS: 8
          NUMBA_DISABLE_JIT: 1
          PYTHONDONTWRITEBYTECODE: 1
          PANDAS_COPY_ON_WRITE: 1
          OPENBLAS_NUM_THREADS: 8
        run: |
          echo "üöÄ STEP 1: SCRAPING AND PROCESSING EVALUATION DATA"
          echo "üìÖ Started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo "üñ•Ô∏è  Runner: ${{ runner.os }}"
          echo "üêç Python version: $(python --version)"
          echo "üíæ Available memory: $(free -h | grep '^Mem:' | awk '{print $2}')"
          echo "‚ö° Processing with optimized sequential chunk processing for stability"
          
          echo ""
          echo "üöÄ BALANCED PARALLEL PROCESSING MODE"
          echo "üì¶ Processing ALL benchmarks simultaneously"
          echo "‚ö° Each benchmark: 8 workers, 100 tasks/chunk, sequential uploads"
          echo "üéØ Target: Reliable processing with reduced resource contention"
          
          pids=()
          for benchmark in lite mmlu classic; do
            echo "üîÑ Starting sequential chunk processing for benchmark: $benchmark"
            (
              python scripts/optimized_helm_processor.py \
                --benchmark "$benchmark" \
                --max-workers 8 \
                --chunk-size 100 \
                --repo-id $HF_REPO_ID
            ) &
            pids+=($!)
            echo "   üìç Started $benchmark with PID: ${pids[-1]}"
            sleep 2
          done

          echo "‚è≥ Waiting for all ${#pids[@]} parallel benchmark processes to complete..."
          echo "üìä Process IDs: ${pids[*]}"

          failed_jobs=()
          for i in "${!pids[@]}"; do
            if wait "${pids[$i]}"; then
              benchmark_name=(lite mmlu classic)
              echo "‚úÖ Benchmark ${benchmark_name[$i]} completed successfully (PID: ${pids[$i]})"
            else
              exit_code=$?
              benchmark_name=(lite mmlu classic)
              echo "‚ùå Benchmark ${benchmark_name[$i]} failed with exit code $exit_code (PID: ${pids[$i]})"
              failed_jobs+=("${benchmark_name[$i]}")
            fi
          done

          if [ ${#failed_jobs[@]} -eq 0 ]; then
            echo ""
            echo "‚úÖ STEP 1 COMPLETED SUCCESSFULLY!"
            echo "üéâ All benchmarks processed successfully in parallel!"
            echo "üìÖ Step 1 completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          else
            echo "‚ö†Ô∏è Some benchmarks failed: ${failed_jobs[*]}"
            echo "üìä Check logs above for specific error details"
            exit 1
          fi

      - name: Upload processing logs as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: processing-logs
          path: |
            *.log
            logs/
          retention-days: 7

  generate_stats:
    name: Step 2 - Generate and Upload Summary Statistics
    runs-on: ubuntu-latest
    needs: [scrape]
    timeout-minutes: 180

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup environment (python, deps, caching)
        uses: ./.github/actions/setup-environment

      - name: Run statistics generation
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO_ID: evaleval/every_eval_ever
          PYTHONUNBUFFERED: 1
        run: |
          echo "üì• Fetching manifests from HuggingFace to speed up stats generation..."
          python -c "import os; from huggingface_hub import hf_hub_download; os.makedirs('data/aggregated', exist_ok=True); repo_id = os.environ.get('HF_REPO_ID'); [print(f'Downloaded manifest for helm_{bm}') if hf_hub_download(repo_id=repo_id, filename=f'manifests/manifest_helm_{bm}.json', repo_type='dataset', local_dir='data/aggregated') or True else print(f'No manifest for helm_{bm}') for bm in ['lite','mmlu','classic']]" || echo "‚ö†Ô∏è Some manifests not found, continuing..."

          echo ""
          echo "üìä STEP 2: GENERATING COMPREHENSIVE DATASET STATISTICS"
          echo "üìÖ Statistics generation started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo "üìã Note: Statistics are generated during the main processing in Step 1"
          echo "‚úÖ Statistics generation completed successfully"
          echo "üìÖ Step 2 completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo ""
          echo "üéä WORKFLOW COMPLETED SUCCESSFULLY!"
