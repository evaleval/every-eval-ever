name: Weekly Evaluation Data Processing and Statistics

on:
  schedule:
    # Weekly: Monday 03:00 UTC
    - cron: '0 3 * * 1'
  workflow_dispatch: {}

jobs:
  scrape:
    name: Step 1 - Scrape and Process HELM Evaluation Data
    runs-on: ubuntu-latest
    timeout-minutes: 1440  # 24 hours - increased for processing all benchmarks in parallel

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup environment (python, deps, caching)
        uses: ./.github/actions/setup-environment

      - name: Run scraping and processing
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO_ID: evaleval/every_eval_ever
          PYTHONUNBUFFERED: 1
          OMP_NUM_THREADS: 8
          NUMBA_DISABLE_JIT: 1
          PYTHONDONTWRITEBYTECODE: 1
          PANDAS_COPY_ON_WRITE: 1
          OPENBLAS_NUM_THREADS: 8
        run: |
          echo "üöÄ STEP 1: SCRAPING AND PROCESSING EVALUATION DATA"
          echo "üìÖ Started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo "üñ•Ô∏è  Runner: ${{ runner.os }}"
          echo "üêç Python version: $(python --version)"
          echo "üíæ Available memory: $(free -h | grep '^Mem:' | awk '{print $2}')"
          echo "‚ö° Processing with simplified global deduplication"
          
          echo ""
          echo "üöÄ SIMPLIFIED PROCESSING MODE"
          echo "ÔøΩ Global deduplication across all benchmarks"
          echo "‚ö° 4 parallel workers, 100 tasks/chunk"
          echo "üéØ Target: Clean architecture with unique chunk numbers"
          
          echo "üîÑ Starting global processing across all benchmarks"
          
          echo "‚è≥ Processing will handle all benchmarks globally..."
          
          if python scripts/simple_helm_processor.py \
            --repo-id "${HF_REPO_ID}" \
            --chunk-size 100 \
            --max-workers 4 \
            --source-name helm; then
            echo ""
            echo "‚úÖ STEP 1 COMPLETED SUCCESSFULLY!"
            echo "üéâ All benchmarks processed successfully with global deduplication!"
            echo "üìÖ Step 1 completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          else
            echo "‚ùå Processing failed"
            echo "üìä Check logs above for specific error details"
            exit 1
          fi

      - name: Upload processing logs as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: processing-logs
          path: |
            *.log
            logs/
          retention-days: 7

  generate_stats:
    name: Step 2 - Generate and Upload Summary Statistics
    runs-on: ubuntu-latest
    needs: [scrape]
    timeout-minutes: 180

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup environment (python, deps, caching)
        uses: ./.github/actions/setup-environment

      - name: Run statistics generation
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO_ID: evaleval/every_eval_ever
          PYTHONUNBUFFERED: 1
        run: |
          echo ""
          echo "üìä STEP 2: GENERATING COMPREHENSIVE DATASET STATISTICS"
          echo "üìÖ Statistics generation started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo "‚úÖ Statistics are generated automatically during processing in Step 1"
          echo "üìÖ Step 2 completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo ""
          echo "üéä WORKFLOW COMPLETED SUCCESSFULLY!"
