name: Weekly Evaluation Data Processing and Statistics

on:
  schedule:
    # Weekly: Monday 03:00 UTC
    - cron: '0 3 * * 1'
  workflow_dispatch: {}

jobs:
  scrape:
    name: Step 1 - Scrape and Process HELM Evaluation Data
    runs-on: ubuntu-latest
    timeout-minutes: 35791  # Maximum possible timeout (~596 hours / ~25 days)

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup environment (python, deps, caching)
        uses: ./.github/actions/setup-environment

      - name: Setup EvalHub Schema
        run: |
          echo "ğŸ“‹ Setting up EvalHub schema..."
          chmod +x setup_schemas.sh
          ./setup_schemas.sh

      - name: Run scraping and processing
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO_ID: evaleval/every_eval_ever
          PYTHONUNBUFFERED: 1
          OMP_NUM_THREADS: 8
          NUMBA_DISABLE_JIT: 1
          PYTHONDONTWRITEBYTECODE: 1
          PANDAS_COPY_ON_WRITE: 1
          OPENBLAS_NUM_THREADS: 8
        run: |
          echo "ğŸš€ STEP 1: SCRAPING AND PROCESSING EVALUATION DATA"
          echo "ğŸ“… Started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo "ğŸ–¥ï¸  Runner: ${{ runner.os }}"
          echo "ğŸ Python version: $(python --version)"
          echo "ğŸ’¾ Available memory: $(free -h | grep '^Mem:' | awk '{print $2}')"
          echo "ğŸ’¿ Disk space: $(df -h / | tail -1 | awk '{print $4}' ) available"
          echo "âš¡ Processing with simplified global deduplication"
          
          echo ""
          echo "ğŸš€ SIMPLIFIED PROCESSING MODE - EvalHub Schema"
          echo "ğŸ“¦ Individual JSON files per evaluation"
          echo "ğŸ“ Nested folder structure: source/benchmark/subject"
          echo "âš¡ STREAMING MODE: 4 parallel workers, 20 tasks/chunk (memory optimized)"
          echo "ğŸ¯ Target: EvalHub-compliant individual evaluation files"
          echo "ğŸŒ Full HELM dataset (all pages) with streaming upload"
          echo "ğŸŒŠ Process -> Upload -> Clean cycle for memory efficiency"
          
          echo "ğŸ”„ Starting EvalHub streaming processing across all benchmarks"
          
          echo "â³ Processing will create, upload, and clean chunks..."
          
          if python scripts/helm_processor.py \
            --repo-id "${HF_REPO_ID}" \
            --chunk-size 20 \
            --max-workers 4 \
            --source-name helm \
            --helm-benchmarks lite classic mmlu; then
            echo ""
            echo "âœ… STEP 1 COMPLETED SUCCESSFULLY!"
            echo "ğŸ‰ All benchmarks processed with streaming EvalHub schema!"
            echo "ğŸŒŠ Data processed, uploaded, and cleaned in memory-efficient chunks"
            echo "ğŸ“… Step 1 completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          else
            echo "âŒ EvalHub processing failed"
            echo "ğŸ“Š Check logs above for specific error details"
            exit 1
          fi

      - name: Upload processing logs as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: processing-logs
          path: |
            *.log
            logs/
          retention-days: 7

  generate_stats:
    name: Step 2 - Generate and Upload Summary Statistics
    runs-on: ubuntu-latest
    needs: [scrape]
    timeout-minutes: 35791  # Maximum possible timeout (~596 hours / ~25 days)

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup environment (python, deps, caching)
        uses: ./.github/actions/setup-environment

      - name: Run statistics generation
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO_ID: evaleval/every_eval_ever
          STATS_REPO_ID: evaleval/every_eval_score_ever
          PYTHONUNBUFFERED: 1
        run: |
          echo ""
          echo "ğŸ“Š STEP 2: GENERATING COMPREHENSIVE DATASET STATISTICS"
          echo "ğŸ“… Statistics generation started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          
          echo "ğŸ“Š Using datasets library for efficient statistics generation"
          
          if python scripts/simple_stats_generator.py \
            --repo-id "${HF_REPO_ID}" \
            --stats-repo-id "${STATS_REPO_ID}" \
            --source-name helm; then
            echo "âœ… Statistics generation completed successfully!"
            echo "ğŸ“Š Generated detailed benchmark+dataset+model statistics"
          else
            echo "âŒ Statistics generation failed"
            exit 1
          fi
          
          echo "ğŸ“… Step 2 completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo ""
          echo "ğŸŠ WORKFLOW COMPLETED SUCCESSFULLY!"
