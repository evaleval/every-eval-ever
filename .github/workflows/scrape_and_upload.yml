name: Weekly Evaluation Data Processing and Statistics

on:
  schedule:
    # Weekly: Monday 03:00 UTC
    - cron: '0 3 * * 1'
  workflow_dispatch: {}

jobs:
  scrape_and_upload:
    name: Process Evaluation Data and Upload to HuggingFace
    runs-on: ubuntu-latest
    timeout-minutes: 720  # Increased to 12 hours for aggressive parallel processing

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install playwright huggingface_hub datasets
          # Install browsers for Playwright (non-interactive, includes deps)
          python -m playwright install --with-deps

      - name: Process evaluation data and upload to HuggingFace (Optimized)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          PYTHONUNBUFFERED: 1
          # Performance optimizations - aggressive settings for maximum speed
          OMP_NUM_THREADS: 8
          NUMBA_DISABLE_JIT: 1
          PYTHONDONTWRITEBYTECODE: 1
          PANDAS_COPY_ON_WRITE: 1
          OPENBLAS_NUM_THREADS: 8
        run: |
          echo "üöÄ Starting optimized chunked data processing workflow"
          echo "üìÖ Started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo "üñ•Ô∏è  Runner: ${{ runner.os }}"
          echo "üêç Python version: $(python --version)"
          echo "üíæ Available memory: $(free -h | grep '^Mem:' | awk '{print $2}')"
          echo "‚ö° Processing with optimized chunked parallel uploads for maximum performance"
          
          # Process benchmarks in parallel for maximum speed
          # This processes all benchmarks simultaneously instead of sequentially
          
          echo ""
          echo "üöÄ AGGRESSIVE PARALLEL PROCESSING MODE"
          echo "üì¶ Processing ALL benchmarks simultaneously"
          echo "‚ö° Each benchmark: 8 workers, 100 tasks/chunk, 6 upload workers"
          echo "üî• Target: Complete entire dataset in under 60 minutes"
          
          # Start all benchmarks in parallel background processes
          # Priority order: lite (fastest), mmlu (medium), classic (largest)
          pids=()
          
          for benchmark in lite mmlu classic; do
            echo "üîÑ Starting parallel processing for benchmark: $benchmark"
            (
              python scripts/optimized_helm_processor.py \
                --benchmark "$benchmark" \
                --workers 8 \
                --chunk-size 100 \
                --upload-workers 6 \
                --chunk-workers 4 \
                --no-verify \
                --batch-size 50 \
                --max-retries 3
            ) &
            pids+=($!)
            echo "   üìç Started $benchmark with PID: ${pids[-1]}"
            # Small delay to stagger startup and reduce initial resource contention
            sleep 2
          done
          
          echo "‚è≥ Waiting for all ${#pids[@]} parallel benchmark processes to complete..."
          echo "üìä Process IDs: ${pids[*]}"
          
          # Wait for all background processes and capture exit codes
          failed_jobs=()
          for i in "${!pids[@]}"; do
            if wait "${pids[$i]}"; then
              benchmark_name=(lite mmlu classic)
              echo "‚úÖ Benchmark ${benchmark_name[$i]} completed successfully (PID: ${pids[$i]})"
            else
              exit_code=$?
              benchmark_name=(lite mmlu classic)
              echo "‚ùå Benchmark ${benchmark_name[$i]} failed with exit code $exit_code (PID: ${pids[$i]})"
              failed_jobs+=("${benchmark_name[$i]}")
            fi
          done
          
          if [ ${#failed_jobs[@]} -eq 0 ]; then
            echo "üéâ All benchmarks processed successfully in parallel!"
            echo "   üìä Processed: lite, mmlu, classic"
            echo "   ‚ö° Used aggressive parallelization (8 workers per benchmark)"
            echo "   üîÄ 4 chunk workers for parallel chunk processing"
            echo "   üì¶ 100 tasks per chunk for optimal parallelization"
            echo "   ‚òÅÔ∏è 6 parallel uploads per benchmark"
            echo "üìÖ Completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          else
            echo "‚ö†Ô∏è Some benchmarks failed: ${failed_jobs[*]}"
            echo "üìä Check logs above for specific error details"
            exit 1
          fi

      - name: Upload processing logs as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: processing-logs
          path: |
            *.log
            logs/
          retention-days: 7

      - name: Generate and upload summary statistics  
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          PYTHONUNBUFFERED: 1
        run: |
          echo "üìä Generating comprehensive dataset statistics..."
          echo "üìÖ Statistics generation started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          
          # Generate stats for all processed benchmarks
          python scripts/optimized_helm_processor.py \
            --benchmark lite \
            --generate-stats-only \
            --workers 8 \
            --upload-workers 6 \
            --no-verify
            
          python scripts/optimized_helm_processor.py \
            --benchmark mmlu \
            --generate-stats-only \
            --workers 8 \
            --upload-workers 6 \
            --no-verify
            
          python scripts/optimized_helm_processor.py \
            --benchmark classic \
            --generate-stats-only \
            --workers 8 \
            --upload-workers 6 \
            --no-verify
          
          echo "‚úÖ Statistics generation completed successfully"
          echo "üìÖ Statistics completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
