name: Weekly Evaluation Data Processing and Statistics

on:
  schedule:
    # Weekly: Monday 03:00 UTC
    - cron: '0 3 * * 1'
  workflow_dispatch: {}

jobs:
  scrape_and_upload:
    name: Process Evaluation Data and Upload to HuggingFace
    runs-on: ubuntu-latest
    timeout-minutes: 720  # Increased to 12 hours for aggressive parallel processing

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install system packages required by Playwright
        run: |
          sudo apt-get update
          sudo apt-get install -y libnss3 libatk1.0-0 libatk-bridge2.0-0 libx11-xcb1 libxcomposite1 libxdamage1 libxrandr2 libgbm1 libxshmfence1 libgtk-3-0

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install playwright huggingface_hub datasets
          # Install browsers for Playwright (non-interactive, includes deps)
          python -m playwright install --with-deps

      - name: Process evaluation data and upload to HuggingFace (Optimized)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          PYTHONUNBUFFERED: 1
          # Performance optimizations - aggressive settings for maximum speed
          OMP_NUM_THREADS: 8
          NUMBA_DISABLE_JIT: 1
          PYTHONDONTWRITEBYTECODE: 1
          PANDAS_COPY_ON_WRITE: 1
          OPENBLAS_NUM_THREADS: 8
        run: |
          echo "ğŸš€ Starting optimized chunked data processing workflow"
          echo "ğŸ“… Started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo "ğŸ–¥ï¸  Runner: ${{ runner.os }}"
          echo "ğŸ”§ Python version: $(python --version)"
          echo "ğŸ’¾ Available memory: $(free -h | grep '^Mem:' | awk '{print $2}' 2>/dev/null || echo 'N/A')"
          echo "ğŸ’¿ Available disk: $(df -h . | tail -1 | awk '{print $4}' 2>/dev/null || echo 'N/A')"
          echo "ğŸ”§ CPU cores: $(nproc 2>/dev/null || echo 'N/A')"
          echo "âš¡ Processing with optimized chunked parallel uploads for maximum performance"
          
          # Process benchmarks in parallel for maximum speed
          # This processes all benchmarks simultaneously instead of sequentially
          
          echo ""
          echo "ğŸš€ AGGRESSIVE PARALLEL PROCESSING MODE"
          echo "ï¿½ Processing ALL benchmarks simultaneously"
          echo "âš¡ Each benchmark: 8 workers, 100 tasks/chunk, 6 upload workers"
          echo "ğŸ”¥ Target: Complete entire dataset in under 60 minutes"
          
          # Start all benchmarks in parallel background processes
          # Priority order: lite (fastest), mmlu (medium), classic (largest)
          pids=()
          
          for benchmark in lite mmlu classic; do
            echo "ğŸ”„ Starting parallel processing for benchmark: $benchmark"
            (
              python scripts/optimized_helm_processor.py \
                --benchmark "$benchmark" \
                --chunk-size 100 \
                --max-workers 8 \
                --upload-workers 6 \
                --timeout 30 \
                --repo-id evaleval/every_eval_ever \
                --source-name helm
              echo "âœ… Completed benchmark: $benchmark"
            ) &
            pids+=($!)
          done
          
          echo ""
          echo "â³ Waiting for all ${#pids[@]} parallel benchmark processes to complete..."
          
          # Wait for all background processes and track failures
          failed=0
          for i in "${!pids[@]}"; do
            if wait "${pids[$i]}"; then
              echo "âœ… Process $((i+1)) completed successfully"
            else
              echo "âŒ Process $((i+1)) failed"
              failed=$((failed+1))
            fi
          done
          
          if [ $failed -eq 0 ]; then
            echo "ğŸ‰ All benchmarks processed successfully in parallel!"
            echo "ğŸ“Š Performance Summary:"
            echo "   âš¡ Used aggressive parallelization (8 workers per benchmark)"
            echo "   ğŸ”„ Processed 3 benchmarks simultaneously"
            echo "   â­ï¸ Skipped already processed tasks (duplicate detection)"
            echo "   ğŸ“¦ 100 tasks per chunk for optimal parallelization"
            echo "   â˜ï¸ 6 parallel uploads per benchmark"
          else
            echo "âš ï¸ $failed out of ${#pids[@]} benchmarks failed"
            exit 1
          fi
          
          echo ""
          echo "ğŸ‰ All benchmarks processed with optimized chunked pipeline!"
          echo "ğŸ“… Completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo "ğŸ“Š Detailed evaluation data uploaded to: evaleval/every_eval_ever"
          echo "ğŸ“ˆ Triggering comprehensive statistics generation..."

  generate_stats_after_scrape:
    name: Generate Comprehensive Statistics After Scraping
    runs-on: ubuntu-latest
    needs: scrape_and_upload
    timeout-minutes: 120

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install huggingface_hub

      - name: Generate comprehensive statistics
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          PYTHONUNBUFFERED: 1
        run: |
          echo "ğŸ“Š Generating comprehensive statistics after successful scraping"
          echo "ğŸ“… Stats generation started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          
          # Generate comprehensive statistics from all newly uploaded data
          python scripts/generate_comprehensive_stats.py \
            --main-repo-id evaleval/every_eval_ever \
            --stats-repo-id evaleval/every_eval_score_ever
          
          echo "âœ… Comprehensive statistics generated successfully"
          echo "ğŸ“… Stats generation completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo "ğŸ“ˆ Statistics uploaded to: evaleval/every_eval_score_ever"
