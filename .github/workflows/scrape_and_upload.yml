name: Weekly Evaluation Data Processing

on:
  schedule:
    # Weekly: Monday 03:00 UTC
    - cron: '0 3 * * 1'
  workflow_dispatch: {}

jobs:
  scrape_and_upload:
    name: Process Evaluation Data and Upload to HuggingFace
    runs-on: ubuntu-latest
    timeout-minutes: 360

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install system packages required by Playwright
        run: |
          sudo apt-get update
          sudo apt-get install -y libnss3 libatk1.0-0 libatk-bridge2.0-0 libx11-xcb1 libxcomposite1 libxdamage1 libxrandr2 libgbm1 libxshmfence1 libgtk-3-0

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install playwright huggingface_hub
          # Install browsers for Playwright (non-interactive, includes deps)
          python -m playwright install --with-deps

      - name: Process evaluation data and upload to HuggingFace (Optimized)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          PYTHONUNBUFFERED: 1
        run: |
          echo "🚀 Starting optimized chunked data processing workflow"
          echo "📅 Started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo "🖥️  Runner: ${{ runner.os }}"
          echo "🔧 Python version: $(python --version)"
          echo "⚡ Processing with chunked parallel uploads for maximum performance"
          
          # Process benchmarks using optimized chunked approach
          # This provides 3-4x speedup and immediate partial results
          
          for benchmark in lite mmlu classic; do
            echo ""
            echo "🔄 Processing benchmark: $benchmark"
            echo "📦 Using chunked processing (200 tasks per chunk for optimal performance)"
            echo "⚡ Parallel processing with 2 workers per chunk"
            echo "☁️ Concurrent uploads while processing continues"
            
            python scripts/optimized_helm_processor.py \
              --benchmark "$benchmark" \
              --chunk-size 200 \
              --max-workers 2 \
              --upload-workers 2 \
              --repo-id evaleval/every_eval_ever \
              --source-name helm
            
            echo "✅ Completed benchmark: $benchmark"
          done
          
          echo ""
          echo "🎉 All benchmarks processed with optimized chunked pipeline!"
          echo "📅 Completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo "📊 Detailed evaluation data uploaded to: evaleval/every_eval_ever"
          echo "📈 Comprehensive statistics will be generated by the daily stats workflow"
          echo "⏰ Stats workflow runs daily at 06:00 UTC"
