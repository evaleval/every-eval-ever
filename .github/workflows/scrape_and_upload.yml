name: Weekly Evaluation Data Processing and Statistics

on:
  schedule:
    # Weekly: Monday 03:00 UTC
    - cron: '0 3 * * 1'
  workflow_dispatch: {}

jobs:
  scrape:
    name: Step 1 - Scrape and Process HELM Evaluation Data
    runs-on: ubuntu-latest
    timeout-minutes: 720

    steps:
      - name: Setup environment (checkout, python, deps)
        uses: ./.github/actions/setup-environment

      - name: Run scraping and processing
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          PYTHONUNBUFFERED: 1
          OMP_NUM_THREADS: 8
          NUMBA_DISABLE_JIT: 1
          PYTHONDONTWRITEBYTECODE: 1
          PANDAS_COPY_ON_WRITE: 1
          OPENBLAS_NUM_THREADS: 8
        run: |
          echo "üöÄ STEP 1: SCRAPING AND PROCESSING EVALUATION DATA"
          echo "üìÖ Started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo "üñ•Ô∏è  Runner: ${{ runner.os }}"
          echo "üêç Python version: $(python --version)"
          echo "üíæ Available memory: $(free -h | grep '^Mem:' | awk '{print $2}')"
          echo "‚ö° Processing with optimized chunked parallel uploads for maximum performance"
          
          echo ""
          echo "üöÄ AGGRESSIVE PARALLEL PROCESSING MODE"
          echo "üì¶ Processing ALL benchmarks simultaneously"
          echo "‚ö° Each benchmark: 8 workers, 100 tasks/chunk, 6 upload workers"
          echo "üî• Target: Complete entire dataset in under 60 minutes"
          
          pids=()
          for benchmark in lite mmlu classic; do
            echo "üîÑ Starting parallel processing for benchmark: $benchmark"
            (
              python scripts/optimized_helm_processor.py \
                --benchmark "$benchmark" \
                --workers 8 \
                --chunk-size 100 \
                --upload-workers 6 \
                --chunk-workers 4 \
                --no-verify \
                --batch-size 50 \
                --max-retries 3
            ) &
            pids+=($!)
            echo "   üìç Started $benchmark with PID: ${pids[-1]}"
            sleep 2
          done

          echo "‚è≥ Waiting for all ${#pids[@]} parallel benchmark processes to complete..."
          echo "üìä Process IDs: ${pids[*]}"

          failed_jobs=()
          for i in "${!pids[@]}"; do
            if wait "${pids[$i]}"; then
              benchmark_name=(lite mmlu classic)
              echo "‚úÖ Benchmark ${benchmark_name[$i]} completed successfully (PID: ${pids[$i]})"
            else
              exit_code=$?
              benchmark_name=(lite mmlu classic)
              echo "‚ùå Benchmark ${benchmark_name[$i]} failed with exit code $exit_code (PID: ${pids[$i]})"
              failed_jobs+=("${benchmark_name[$i]}")
            fi
          done

          if [ ${#failed_jobs[@]} -eq 0 ]; then
            echo ""
            echo "‚úÖ STEP 1 COMPLETED SUCCESSFULLY!"
            echo "üéâ All benchmarks processed successfully in parallel!"
            echo "üìÖ Step 1 completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          else
            echo "‚ö†Ô∏è Some benchmarks failed: ${failed_jobs[*]}"
            echo "üìä Check logs above for specific error details"
            exit 1
          fi

      - name: Upload processing logs as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: processing-logs
          path: |
            *.log
            logs/
          retention-days: 7

  generate_stats:
    name: Step 2 - Generate and Upload Summary Statistics
    runs-on: ubuntu-latest
    needs: [scrape]
    timeout-minutes: 180

    steps:
      - name: Setup environment (checkout, python, deps)
        uses: ./.github/actions/setup-environment

      - name: Run statistics generation
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          PYTHONUNBUFFERED: 1
        run: |
      echo "üì• Fetching manifests from HuggingFace to speed up stats generation..."
      python - <<'PY'
import os
from huggingface_hub import HfApi, hf_hub_download
os.makedirs('data/aggregated', exist_ok=True)
api = HfApi(token=os.environ.get('HF_TOKEN'))
for bm in ['lite','mmlu','classic']:
  try:
    # Download manifest if available
    hf_hub_download(repo_id='${{ secrets.REPO_ID }}' if False else os.environ.get('HF_REPO_ID',''), filename=f'manifests/manifest_{bm}.json', repo_type='dataset', local_dir='data/aggregated')
    print(f'Downloaded manifest for {bm}')
  except Exception as e:
    print(f'No manifest for {bm}: {e}')
PY

          echo ""
          echo "üìä STEP 2: GENERATING COMPREHENSIVE DATASET STATISTICS"
          echo "üìÖ Statistics generation started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"

          python scripts/optimized_helm_processor.py \
            --benchmark lite \
            --generate-stats-only \
            --workers 8 \
            --upload-workers 6 \
            --no-verify

          python scripts/optimized_helm_processor.py \
            --benchmark mmlu \
            --generate-stats-only \
            --workers 8 \
            --upload-workers 6 \
            --no-verify

          python scripts/optimized_helm_processor.py \
            --benchmark classic \
            --generate-stats-only \
            --workers 8 \
            --upload-workers 6 \
            --no-verify

          echo "‚úÖ Statistics generation completed successfully"
          echo "üìÖ Step 2 completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo ""
          echo "üéä WORKFLOW COMPLETED SUCCESSFULLY!"
