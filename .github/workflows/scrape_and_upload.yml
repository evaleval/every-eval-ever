name: Weekly HELM scrape and upload

on:
  schedule:
    # Weekly: Monday 03:00 UTC
    - cron: '0 3 * * 1'
  workflow_dispatch: {}

jobs:
  validate_env:
    name: Validate environment (quick test)
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install minimal system packages
        run: |
          sudo apt-get update
          sudo apt-get install -y libnss3 libatk1.0-0 libatk-bridge2.0-0 libx11-xcb1 libxcomposite1 libxdamage1 libxrandr2 libgbm1 libxshmfence1 libgtk-3-0 || true

      - name: Install Python dependencies (quick)
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install playwright huggingface_hub

      - name: Quick import check
        run: |
          python - <<'PY'
          try:
              import sys
              import pandas
              import pyarrow
              import playwright
              import huggingface_hub
              print('import check OK')
          except Exception as e:
              print('Import check failed:', e)
              raise
          PY

  scrape_and_upload:
    name: Scrape HELM and upload to Hugging Face Dataset
    needs: validate_env
    runs-on: ubuntu-latest
    timeout-minutes: 360

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install system packages required by Playwright
        run: |
          sudo apt-get update
          sudo apt-get install -y libnss3 libatk1.0-0 libatk-bridge2.0-0 libx11-xcb1 libxcomposite1 libxdamage1 libxrandr2 libgbm1 libxshmfence1 libgtk-3-0

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install playwright huggingface_hub
          # Install browsers for Playwright (non-interactive, includes deps)
          python -m playwright install --with-deps

      - name: Run incremental scrape with live uploads
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - <<'PY'
          import os
          import re
          import subprocess
          import sys
          from pathlib import Path
          from datetime import datetime
          from huggingface_hub import HfApi

          # Initialize HF API
          token = os.environ.get('HF_TOKEN')
          if not token:
              raise SystemExit('HF_TOKEN not set in environment (provide via repo secrets)')

          repo_id = 'evaleval/every_eval_ever'
          api = HfApi()

          # Ensure dataset repo exists
          try:
              api.create_repo(repo_id=repo_id, repo_type='dataset', token=token)
              print(f'Dataset repo ready: {repo_id}')
          except Exception as e:
              print(f'Repo setup: {e}')

          # Find the highest existing part number in the dataset
          try:
              repo_files = api.list_repo_files(repo_id=repo_id, repo_type='dataset')
              part_numbers = []
              for file in repo_files:
                  # Look for files matching pattern: data-XXXXX.parquet
                  match = re.match(r'data-(\d+)\.parquet$', file)
                  if match:
                      part_numbers.append(int(match.group(1)))
              
              if part_numbers:
                  next_part_num = max(part_numbers) + 1
                  print(f'Found existing parts up to {max(part_numbers)}, starting from part {next_part_num}')
              else:
                  next_part_num = 1
                  print('No existing parts found, starting from part 1')
          except Exception as e:
              print(f'Error checking existing files, starting from part 1: {e}')
              next_part_num = 1

          # Default benchmarks to process incrementally
          benchmarks = ["lite", "mmlu", "classic"]
          source_name = "helm"  # Current source is HELM, future sources can be added here
          timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
          
          for i, benchmark in enumerate(benchmarks):
              current_part_num = next_part_num + i
              print(f"\n=== Processing benchmark {i+1}/{len(benchmarks)}: {benchmark} from {source_name} (part {current_part_num}) ===")
              
              try:
                  # Create shard filename with incremental numbering
                  # Using simple incremental format that's compatible with HF datasets
                  shard_name = f"data-{current_part_num:05d}.parquet"
                  output_path = Path("data/aggregated") / shard_name
                  output_path.parent.mkdir(parents=True, exist_ok=True)
                  
                  # Run scraper for this specific benchmark
                  cmd = [
                      sys.executable, "-m", "src.core.scrape_all_to_parquet",
                      "--output", str(output_path),
                      "--benchmarks", benchmark,
                      "--overwrite",
                      "--max-workers", "2"
                  ]
                  
                  print(f"Running: {' '.join(cmd)}")
                  result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)  # 1 hour timeout per benchmark
                  
                  if result.returncode != 0:
                      print(f"ERROR processing {benchmark}:")
                      print(f"STDOUT: {result.stdout}")
                      print(f"STDERR: {result.stderr}")
                      continue
                  
                  print(f"Successfully processed {benchmark}")
                  
                  # Upload the shard immediately
                  if output_path.exists():
                      print(f"Uploading shard: {shard_name}")
                      api.upload_file(
                          path_or_fileobj=str(output_path),
                          path_in_repo=shard_name,
                          repo_id=repo_id,
                          repo_type='dataset',
                          token=token,
                      )
                      print(f"✓ Uploaded {shard_name} to dataset")
                      
                      # Clean up local file to save space
                      output_path.unlink()
                      print(f"✓ Cleaned up local file {shard_name}")
                      
                  else:
                      print(f"WARNING: Output file not found: {output_path}")
                      
              except subprocess.TimeoutExpired:
                  print(f"TIMEOUT: {benchmark} took longer than 1 hour, skipping")
              except Exception as e:
                  print(f"ERROR processing {benchmark}: {e}")
                  
          print(f"\n=== Incremental upload complete. Next run will start from part {next_part_num + len(benchmarks)} ===")
          PY