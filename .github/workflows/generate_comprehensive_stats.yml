name: Generate Comprehensive Statistics (Standalone)

on:
  schedule:
    # Daily at 06:00 UTC (fallback if post-scrape stats fail)
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      force_regenerate:
        description: 'Force regenerate even if recent stats exist'
        required: false
        default: false
        type: boolean

jobs:
  generate_comprehensive_stats:
    name: Generate Comprehensive Statistics (Standalone/Fallback)
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install huggingface_hub datasets

      - name: Generate comprehensive statistics
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          PYTHONUNBUFFERED: 1
        run: |
          # Generate comprehensive statistics using datasets library directly
          # Much more efficient and reliable than manual shard processing
          
          echo "🚀 Starting comprehensive statistics generation for all sources"
          echo "📝 Using datasets library for efficient data loading"
          echo "📅 Started at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"
          echo "🖥️  Runner: ${{ runner.os }}"
          echo "📊 This will load the complete dataset using datasets library"
          echo "⚡ Memory efficient: leverages datasets streaming capabilities"
          
          python scripts/simple_stats_generator.py \
            --repo-id evaleval/every_eval_ever \
            --stats-repo-id evaleval/every_eval_score_ever \
            --source-name helm
          
          echo "📅 Completed at: $(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")"

      - name: Validate statistics output
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          # Quick validation that statistics were generated successfully
          python -c "
          import os
          from huggingface_hub import HfApi, list_repo_files
          
          token = os.environ.get('HF_TOKEN')
          if not token:
              print('❌ HF_TOKEN not available for validation')
              exit(1)
          
          try:
              api = HfApi()
              files = list_repo_files(
                  repo_id='evaleval/every_eval_score_ever',
                  repo_type='dataset'
              )
              
              # Look for detailed_statistics.parquet and detailed_metadata.json files
              stats_file = 'detailed_statistics.parquet'
              metadata_file = 'detailed_metadata.json'
              
              if stats_file in files and metadata_file in files:
                  print(f'✅ Found comprehensive statistics files:')
                  print(f'   📄 {stats_file}')
                  print(f'   📄 {metadata_file}')
                  print('🎉 Statistics generation completed successfully!')
              else:
                  print('⚠️ Expected statistics files not found')
                  print(f'📁 Available files: {files}')
                  
          except Exception as e:
              print(f'❌ Validation failed: {e}')
              exit(1)
          "

  notify_completion:
    name: Notify Statistics Generation Completion
    needs: generate_comprehensive_stats
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Report results
        run: |
          if [ "${{ needs.generate_comprehensive_stats.result }}" == "success" ]; then
            echo "🎉 Comprehensive statistics generation completed successfully!"
            echo "📊 Statistics are now available at: evaleval/every_eval_score_ever"
            echo "🔄 Next scheduled run: Tomorrow at 06:00 UTC"
          else
            echo "❌ Comprehensive statistics generation failed"
            echo "🔧 Check the logs above for details"
            echo "💡 You can manually trigger this workflow to retry"
          fi
